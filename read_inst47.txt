# instructions47 — Final polish for DevPost submission (AAS v8 → submission-ready)

Owner: **Antigravity**  
Goal: make the **deployed** AAS build unmistakably “real”, demo‑reliable, and judge‑ready, by wiring the new backend capabilities end‑to‑end, surfacing them in the UI, and tightening submission materials.

## Success criteria (what “done” looks like)
1. **Real execution is visible** in the demo (Salesforce task + Slack message) *or* the UI clearly states when it is running in demo mode.
2. Actions are **ranked by impact** and show a 1‑line **AI rationale**.
3. The UI shows **impact metrics** for each run (even as a small summary block).
4. DevPost assets are consistent: README + demo script + env setup + screenshots.

---

## 1) Sync Antigravity backend work into the v8 code line (or confirm it’s already there)

### A. Salesforce + env var consistency
- Ensure **one canonical set** of env vars across code + docs:
  - `SF_USERNAME`, `SF_PASSWORD`, `SF_SECURITY_TOKEN` (preferred)
  - `SLACK_BOT_TOKEN`
  - `OPENAI_API_KEY`
- Update any existing variants (e.g., `SALESFORCE_USERNAME`) to map to the canonical names (support aliases if you want, but docs must be canonical).

### B. Implement/confirm real `SalesforceClient`
- File: `aas/services/salesforce_client.py`
  - Uses `simple_salesforce.Salesforce`.
  - Implements `create_task(subject, description, owner_id, what_id)`.
  - Returns `{ "task_id": "...", "status": "created" }`.
  - If credentials missing → return a **demo mode** result, not an exception.

### C. Executor returns real identifiers
- File: `aas/executor.py`
  - When creating Salesforce tasks, ensure response includes a real `task_id` (or demo placeholder).

---

## 2) Surface Impact Quantification (backend + UI)

### A. Backend: compute run metrics
- In `PipelineLeakageAgent.analyze()` (and inherited plays):
  - Produce `analysis.metrics` including (example names ok):
    - `value_at_risk`
    - `expected_revenue_recovered`
    - `num_at_risk`
    - `avg_stage_age`
  - Ensure these are returned by the API and appear in the `/run/{play}` response.

### B. Backend: action impact_score
- Update `Action` model:
  - Add `impact_score: float` (default 0)
  - Add `reasoning: str | None` (optional)
- Sort actions by `impact_score` before returning.

### C. Frontend: display metrics + impact sorting
- File: `web/app.js` + `web/index.html`:
  - Add a small “Run summary” block above the action list showing metrics.
  - On each action card show:
    - **Impact** (e.g., `$12.4k at risk` or `Impact score: 0.82`)
    - **AI rationale** (1 line)
  - If no OpenAI key, show a deterministic fallback rationale (e.g., from rules).

---

## 3) “Wow” layer: AI rationale generation

### A. Backend
- Base class `AgentPlay` (or common util): add `generate_rationale(action, analysis)`
  - If `OPENAI_API_KEY` present: call OpenAI and return a single sentence.
  - Else: return rule-based rationale string.
- Attach result to `Action.reasoning`.

### B. Guardrails
- Ensure the rationale never leaks secrets or raw tokens.
- Add a simple max length (e.g., 140 chars) so UI stays clean.

---

## 4) Feedback loop logging (judge-visible)

Goal: make the “closed loop” tangible.

### A. Backend
- On `/approve`, append approved actions into `data/action_feedback_log.csv` with columns:
  - `timestamp, run_id, play, action_id, type, priority, impact_score, approved_by`
- Keep existing JSONL logging if present; CSV is the “easy to show” artifact.

### B. Frontend (optional but helpful)
- After approve, show a toast that mentions feedback logging:
  - “Approved 10 actions (logged to feedback CSV).”

---

## 5) Demo reliability and narrative consistency

### A. Update runbook language
- Update `README_DEMO.md` to remove claims like “50,000 Salesforce records and Slack chatter” unless demonstrably true.
- Make the demo mode explicit:
  - “If SF/Slack keys missing, we simulate execution but still log approvals.”

### B. Fix small technical debt that can embarrass the demo
- `TableauClient` currently has **duplicate `get_views`** definitions — remove/merge to one.
- Confirm `/tableau/jwt` returns correct `vizUrl` per play and never 500s silently.

---

## 6) DevPost submission pack checklist (what we need ready)

1. **1–2 sentence pitch** (tight).
2. **<5 minute demo video** matching the updated demo plan.
3. **Screenshots** (3–5) that match the hero plays.
4. **Repo link** + clear “Run locally” + env var setup via `.env.example`.
5. If repo private: grant Devpost reviewers access (per rules).

---

## QA / Acceptance tests

### Smoke test (local)
1. `uvicorn aas.api:app --reload` + serve `web/`.
2. Run Pipeline play → action cards populate with impact + rationale.
3. Approve All → execution_results include Salesforce `task_id` (real or demo).
4. Confirm `data/action_feedback_log.csv` is created/appended.

### Demo rehearsal (Netlify)
1. Load deployed URL with cache bust.
2. Run each play once.
3. Select a bar in viz → context badge updates + actions filter.
4. Approve All → list clears + success toast.

---

## Deliverables
- PR merged to `main`.
- Updated `.env.example` and README/README_DEMO.
- UI shows metrics, impact, and rationale.
- CSV feedback log created on approvals.

